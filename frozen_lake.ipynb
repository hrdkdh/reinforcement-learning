{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "input_size = env.observation_space.n\n",
    "output_size = env.action_space.n\n",
    "discount_rate = 0.99\n",
    "num_episodes = 1000\n",
    "r_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(1, 16)))\n",
    "model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(output_size, activation=\"softmax\"))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    loss = \"mse\",\n",
    "    optimizer = optimizer,\n",
    "    metrics = [\"mae\", \"mse\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(s_no):\n",
    "    ret = np.zeros((1, 16), dtype=np.float32)\n",
    "    ret[0, s_no] = 1\n",
    "    ret = np.expand_dims(ret, 1)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_cnt_list = []\n",
    "for i in range(0, num_episodes): #에피소드가 진행되면서 model도 계속 업데이트 된다!!!\n",
    "    s = env.reset()\n",
    "    e = 1. / ((i / 50) +10)\n",
    "    r_all = 0\n",
    "    done = False\n",
    "    local_loss = []\n",
    "\n",
    "    action_cnt = 0\n",
    "    while not done:\n",
    "        action_cnt += 1\n",
    "        print(\"에피소드 : {}/{} ({}%), action_cnt : {}  \".format(i+1, num_episodes, (100*(i+1))/num_episodes, action_cnt), end=\"\\r\")\n",
    "        Qs = model.predict(onehot(s)) #전혀 훈련되지 않은 채로 일단 predict\n",
    "        #Qs : [0.12, 0.29, 0.01, 0.05]\n",
    "        if np.random.rand(1) < e:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            a = np.argmax(Qs) #4개 액션 중 가장 큰 값을 가진 녀석의 index no를 알려줄 것\n",
    "        \n",
    "        s1, reward, done, _ = env.step(a) #이동해 본다.\n",
    "        if done: #목적지에 다다르면\n",
    "            Qs[0, a] = reward #predict를 통해 나온 4개 액션에 reward를 입력 -> Y라벨을 업데이트 하는 것\n",
    "        else:\n",
    "            Qs1 = model.predict(onehot(s1)) #현재 state를 input으로 하여 다시 predict를 해서 다음 action을 구하는 것\n",
    "            Qs[0, a] = reward + discount_rate*np.max(Qs1) #predict를 통해 나온 4개 액션에 reward를 입력하는 것 -> Y라벨을 업데이트 하는 것\n",
    "        \n",
    "        model.fit(x=onehot(s), y=Qs, verbose=False)\n",
    "        r_all += reward #이번 에피소드의 총보상값 업데이트\n",
    "        s = s1\n",
    "    r_list.append(r_all)\n",
    "    action_cnt_list.append(action_cnt)\n",
    "\n",
    "print(\"Percent of successful episode: {}\".format((100*sum(r_list))/num_episodes)+\"%\")\n",
    "plt.bar(range(len(r_list)), r_list, color='blue')\n",
    "plt.show()\n",
    "\n",
    "action_cnt_df = pd.DataFrame(action_cnt_list)\n",
    "action_cnt_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aaf533d5ae3a3d7a7fcdd7d995dbe2f3fcb3d854cc4805079aca601e58923c31"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv_tf_3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
